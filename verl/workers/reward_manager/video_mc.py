# video-verl/video.py
import logging
from typing import Tuple, Optional

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def temporal_iou(interval_1: Tuple[float, float], interval_2: Tuple[float, float]) -> float:
    """
    Calculate the Intersection over Union (IoU) between two temporal intervals.

    Args:
        interval_1: Tuple (start_time, end_time).
        interval_2: Tuple (start_time, end_time).

    Returns:
        The IoU score between 0.0 and 1.0.
    """
    start_1, end_1 = interval_1
    start_2, end_2 = interval_2

    # Ensure intervals are valid
    if start_1 > end_1:
        logger.warning(f"Invalid interval_1: start {start_1} > end {end_1}. Swapping.")
        start_1, end_1 = end_1, start_1
    if start_2 > end_2:
        logger.warning(f"Invalid interval_2: start {start_2} > end {end_2}. Swapping.")
        start_2, end_2 = end_2, start_2

    # Calculate intersection
    intersection_start = max(start_1, start_2)
    intersection_end = min(end_1, end_2)
    intersection_duration = max(0.0, intersection_end - intersection_start)

    # Calculate union
    union_start = min(start_1, start_2)
    union_end = max(end_1, end_2)
    union_duration = union_end - union_start

    # Calculate IoU
    if union_duration <= 0.0:
        # Avoid division by zero; if union is zero, IoU is 0 unless intersection is also 0
        # If both intervals have zero length and overlap perfectly, IoU could be 1.
        # Handle this edge case based on specific requirements. For now, return 0 if union is 0.
        return 0.0

    iou = intersection_duration / union_duration
    return iou


def calculate_multistage_reward(
    predicted_interval: Optional[Tuple[float, float]],
    generated_answer: Optional[str],
    gt_interval: Optional[Tuple[float, float]],
    gt_answer: Optional[str],
    alpha: float = 0.5, # Weight for IoU
    beta: float = 0.5,  # Weight for QA Match
    iou_threshold: Optional[float] = None # Optional: Binary reward based on IoU threshold
) -> float:
    """
    Calculates the combined IoU and QA match reward for the multi-stage pipeline.

    Args:
        predicted_interval: The interval [start, end] predicted by Stage 1.
        generated_answer: The answer text generated by Stage 3.
        gt_interval: The ground truth interval [start, end].
        gt_answer: The ground truth answer text.
        alpha: Weight for the IoU component.
        beta: Weight for the QA match component.
        iou_threshold: If set, IoU reward becomes 1 if IoU >= threshold, else 0.

    Returns:
        The calculated scalar reward.
    """
    iou_score = 0.0
    if gt_interval and predicted_interval:
        # Ensure GT interval components are valid numbers
        gt_start, gt_end = gt_interval
        pred_start, pred_end = predicted_interval

        # Check for None or non-numeric values before calculating IoU
        if all(isinstance(t, (int, float)) for t in [gt_start, gt_end, pred_start, pred_end]):
            iou_score = temporal_iou((pred_start, pred_end), (gt_start, gt_end))
        else:
             logger.warning(f"Skipping IoU calculation due to invalid time values: "
                           f"Pred={predicted_interval}, GT={gt_interval}")

    # Apply IoU threshold if provided
    if iou_threshold is not None:
        iou_reward = 1.0 if iou_score >= iou_threshold else 0.0
    else:
        iou_reward = iou_score # Use raw IoU score

    # Calculate QA Match (Exact Match, case-insensitive)
    qa_match_reward = 0.0
    if gt_answer is not None and generated_answer is not None:
        # Normalize strings for comparison (lowercase, strip whitespace)
        qa_match_reward = 1.0 if generated_answer.strip().lower() == gt_answer.strip().lower() else 0.0
    elif gt_answer is None and generated_answer is None:
         # If no GT answer expected and none generated, consider it a match? Or 0?
         # Depends on task definition. Let's assume 0 if no GT answer.
         pass


    # Combine rewards - Ensure weights sum to 1 or normalize if needed
    # If alpha + beta != 1, the reward scale might change.
    # Consider normalizing: total_weight = alpha + beta; reward = (alpha * iou_reward + beta * qa_match_reward) / total_weight if total_weight > 0 else 0
    reward = alpha * iou_reward + beta * qa_match_reward

    return reward


# --- Original TVGReward class ---
# This class might be deprecated if the reward logic is handled directly
# within the RayPPOTrainer._compute_rewards method, which might be simpler
# for accessing config parameters like alpha and beta.
# Keep it for reference or if you prefer a class-based approach.

# class TVGReward:
#     def __init__(self, format_weight=0.5, iou_weight=0.5, iou_threshold=0.5):
#         self.format_weight = format_weight
#         self.iou_weight = iou_weight
#         self.iou_threshold = iou_threshold
#         logger.info(f"TVGReward initialized with format_weight={format_weight}, iou_weight={iou_weight}, iou_threshold={iou_threshold}")

#     def tvg_reward(self, generated_text, gt_interval):
#         # ... (Original logic - needs significant update for multi-stage) ...
#         # This logic needs to be replaced by calculate_multistage_reward
#         # or similar logic within the trainer.
#         logger.warning("Original tvg_reward function is likely outdated for the multi-stage pipeline.")
#         return 0.0 # Placeholder

#     def _parse_generated_text(self, text):
#         # ... (Original parsing logic - might be needed for _parse_localization_output) ...
#         pass

